{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "add4d35e-048b-4bc7-8bc5-d75b48d37466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import text\n",
    "import dataframe_image as dfi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eeb39ed2-10fa-40e8-bd6e-e5854fb82a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ufc_mma_submissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dca398db-b6c4-4835-9695-8cffb4a962d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title']\n",
    "y = df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4119d1a6-4976-4a5d-b539-8cce0c134ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95, 0.98],\n",
       "                         'cvec__max_features': [5000, 6000, 7000, 8000, 9000,\n",
       "                                                10000, 11000, 12000],\n",
       "                         'cvec__min_df': [2, 3],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'cvec__stop_words': [None, 'english']})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1 = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe1_params = {\n",
    "    'cvec__max_features' : [5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__stop_words': [None, 'english'],\n",
    "    'cvec__max_df' : [0.9, 0.95, 0.98],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)]\n",
    "}\n",
    "\n",
    "gs_pipe1 = GridSearchCV(pipe1,\n",
    "                        param_grid=pipe1_params,\n",
    "                        cv = 5,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "gs_pipe1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "425c29b9-ed44-4c93-baf5-f2e20ef3a97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7417417216561439\n",
      "Best Params: {'cvec__max_df': 0.9, 'cvec__max_features': 10000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': None}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best Score: {gs_pipe1.best_score_}')\n",
    "print(f'Best Params: {gs_pipe1.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b344975-e027-4080-b904-4d99d03181ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8344608038201353\n",
      "Testing Score: 0.749005568814638\n"
     ]
    }
   ],
   "source": [
    "print(f'Training Score: {gs_pipe1.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {gs_pipe1.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d4f097-563e-4f57-b943-af02cd704d4d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "178f5411-0934-4e4d-ad3a-eb05843f003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title']\n",
    "y = df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c7e3706-d395-46ac-8b4d-17602a995388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'tvec__max_df': [0.9, 0.95, 0.98],\n",
       "                         'tvec__max_features': [5000, 6000, 7000, 8000, 9000,\n",
       "                                                10000, 11000, 12000],\n",
       "                         'tvec__min_df': [2, 3],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'tvec__stop_words': [None, 'english']})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe2_params = {\n",
    "    'tvec__max_features' : [5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__max_df': [0.9, 0.95, 0.98],\n",
    "    'tvec__min_df': [2, 3]\n",
    "}\n",
    "\n",
    "gs_pipe2 = GridSearchCV(pipe2,\n",
    "                        param_grid=pipe2_params,\n",
    "                        cv = 5,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "gs_pipe2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa27f904-b774-4682-b997-b3d8ebac6e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7482417154956797\n",
      "Best Params: {'tvec__max_df': 0.9, 'tvec__max_features': 6000, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': None}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best Score: {gs_pipe2.best_score_}')\n",
    "print(f'Best Params: {gs_pipe2.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "642c370a-ab78-45a9-9d1c-cf790d208da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8194720785250034\n",
      "Testing Score: 0.7474144789180589\n"
     ]
    }
   ],
   "source": [
    "print(f'Training Score: {gs_pipe2.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {gs_pipe2.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd1e93e-7735-4295-a69b-dc17ab58c716",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Top Words Before Adding Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "106f485b-2444-47a4-9b6e-7078b42616c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(max_features=10000,\n",
    "                             max_df = 0.9,\n",
    "                             min_df = 2,\n",
    "                             ngram_range=(1,2),\n",
    ")\n",
    "vectorized = cvec.fit_transform(X)\n",
    "vectorized = pd.DataFrame(vectorized.todense(), columns = cvec.get_feature_names_out())\n",
    "top_ufc = vectorized[df['subreddit'] == 1].sum().sort_values(ascending = False)[:20]\n",
    "top_mma = vectorized[df['subreddit'] == 0].sum().sort_values(ascending = False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9255c22-b96d-46ce-968e-0c3f2227ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_index = []\n",
    "ufc_count = []\n",
    "mma_count = []\n",
    "for i in top_ufc.index:\n",
    "    if i in top_mma.index:\n",
    "        count_index.append(i)\n",
    "        ufc_count.append(top_ufc[i])\n",
    "        mma_count.append(top_mma[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c3838ae-82ba-4969-bcaa-2bea17194b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = pd.DataFrame(index = count_index)\n",
    "top_words['ufc'] = ufc_count\n",
    "top_words['mma'] = mma_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad2ae371-9204-442c-8d5c-8a3f44a069f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ufc</th>\n",
       "      <th>mma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>2111</td>\n",
       "      <td>1681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>1104</td>\n",
       "      <td>1148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ufc</th>\n",
       "      <td>877</td>\n",
       "      <td>1130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>867</td>\n",
       "      <td>549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>809</td>\n",
       "      <td>939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>744</td>\n",
       "      <td>707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>734</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>725</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fight</th>\n",
       "      <td>607</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>579</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>505</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>477</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>471</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>his</th>\n",
       "      <td>336</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ufc   mma\n",
       "the    2111  1681\n",
       "to     1104  1148\n",
       "ufc     877  1130\n",
       "is      867   549\n",
       "in      809   939\n",
       "and     744   707\n",
       "of      734   759\n",
       "this    725   260\n",
       "fight   607   725\n",
       "for     579   662\n",
       "you     505   249\n",
       "on      477   697\n",
       "he      471   298\n",
       "his     336   368"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfi.export(top_words, \"../images/topwords_no_stop_words.png\")\n",
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96e1e12-dc63-475b-a8a1-bcbcfdddcf6e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810fa926-67ed-4914-a00b-2f3ba25b9280",
   "metadata": {},
   "source": [
    "### Custom Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a2fe4ac-9c06-4291-89eb-1fbbca4459bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_words_list= (['ufc', 'dana', 'white', 'ultimate', 'u.f.c.', 'islam', 'makhachev', 'moreno', 'edwards', 'usman', 'ngannou', 'adesanya',\n",
    "                  'pantoja', 'kara', 'kai', 'oliveira', 'pereira', 'sterling', 'royval', 'nicolau', 'perez', 'albazi', 'schnell', 'omalley', 'yan',\n",
    "                  'dvalishvili', 'vera', 'sandhagen', 'font', 'cruz', 'holloway', 'volkanovski', 'figueiredo', 'deiveson', 'aljamain', 'rodriguez',\n",
    "                  'ortega', 'allen', 'emmett', 'chan', 'sung', 'jung', 'kattar', 'giga', 'chikadze', 'poirier', 'jones', 'elliott', 'dvorak', 'molina', 'mokaev',\n",
    "                  'ulanbekov', 'yanez', 'gutierrez', 'nurmagomedov', 'simon', 'munhoz', 'shore', 'topuria', 'evloev', 'mitchell', 'yusuff', 'iga', 'barboza',\n",
    "                  'caceres', 'burns', 'neal', 'luque', 'fiziev', 'gamrot', 'anjos', 'tsarukyan', 'turner', 'hooker', 'ismagulov', 'gaethje', 'magny', 'whittaker',\n",
    "                  'vettori', 'strickland', 'costa', 'hermansson', 'covington', 'muniz', 'imavov', 'bachowicz', 'rakic', 'cannonier', 'dolidze', 'brunson', 'oezdemir',\n",
    "                  'spann', 'walker', 'nunes', 'weili', 'shevchenko', 'pena', 'blaydes', 'tuivasa', 'aspinall', 'andrade', 'santos', 'daukaus', 'tybura', 'lewis', 'holm',\n",
    "                  'vieira', 'jandiroba', 'maia', 'grasso', 'chookagian', 'murphy', 'fiorot', 'lemos', 'namajunas', 'esparza', 'jandiroba', 'blanchfield', 'barber',\n",
    "                  'calvillo', 'ribas', 'viana', 'ducote', 'pinheiro', 'xiaonan', 'yan', 'abdurakhimov', 'spivac', 'shamil', 'ketlen', 'pennington', 'miesha', 'kunitskaya',\n",
    "                  'rosa', 'avila', 'lansberg', 'paddy', 'silva', 'cormier', 'diaz', 'miocic', 'lesnar', 'penn', 'liddell', 'pierre', 'rousey', 'khabib', 'conor', 'mcgregor',\n",
    "                  'frevola', 'dillashaw', 'pimblett', 'helwani', 'blachowicz','arlovski', 'donatello', 'dec', 'december', 'jan', 'feb', 'selftext', 'says', 'did', 'does',\n",
    "                  'guy', 'guys', 'know', 'fc', 'vs', 'https', 'khamzat', '2022', '2023', '219', '281', '282', '283', '284', '285', 'going', 'man', 'got', 'anne', 'didnt', \n",
    "                  'ufc281', 'ankalaev', 'zhang', 'israel', 'johnson', 'dustin', 'krause', 'chandler', 'jiri', 'cejudo', 'march', 'februrary', 'gordon', 'ilia', 'florian',\n",
    "                  'makachov', 'beneil', 'dariush', 'jared'])\n",
    "stop_words_list = text.ENGLISH_STOP_WORDS.union(my_words_list)\n",
    "\n",
    "# Got help with this combination from https://stackoverflow.com/questions/26826002/adding-words-to-stop-words-list-in-tfidfvectorizer-in-sklearn\n",
    "# UFC fighters taken from https://www.ufc.com/rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "080d62ae-431a-4abd-a127-ae9e9af5cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title']\n",
    "y = df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7ffecf3-8378-4776-9055-384ed22aba03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7080514627582335\n",
      "Best Params: {'cvec__max_df': 0.1, 'cvec__max_features': 4000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 1)}\n",
      "Training Score: 0.8009019763894416\n",
      "Testing Score: 0.7163882259347654\n"
     ]
    }
   ],
   "source": [
    "pipe4 = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words_list)),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe4_params = {\n",
    "    'cvec__max_features': [3000, 4000, 5000],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__max_df': [0.01, 0.05, 0.1],\n",
    "    'cvec__min_df': [2, 3],\n",
    "}\n",
    "\n",
    "gs_pipe4 = GridSearchCV(pipe4,\n",
    "                        param_grid=pipe4_params,\n",
    "                        cv = 5)\n",
    "\n",
    "gs_pipe4.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best Score: {gs_pipe4.best_score_}')\n",
    "print(f'Best Params: {gs_pipe4.best_params_}')\n",
    "print(f'Training Score: {gs_pipe4.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {gs_pipe4.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a446aae6-f402-4718-84c4-df0a4562e1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7107037186322361\n",
      "Best Params: {'tvec__max_df': 0.8, 'tvec__max_features': 4000, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "pipe3 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer(stop_words=stop_words_list)),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe3_params = {\n",
    "    'tvec__max_features': [2000, 4000, 6000, 8000, 10000],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__max_df': [0.8, 0.9, 0.95, 0.98, 1.0],\n",
    "    'tvec__min_df': [2, 3, 4],\n",
    "}\n",
    "\n",
    "gs_pipe3 = GridSearchCV(pipe3,\n",
    "                        param_grid=pipe3_params,\n",
    "                        cv = 5)\n",
    "\n",
    "gs_pipe3.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best Score: {gs_pipe3.best_score_}')\n",
    "print(f'Best Params: {gs_pipe3.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4303cda-13e5-44f3-8948-d2c709c99f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8180129990714949\n",
      "Testing Score: 0.7183770883054893\n"
     ]
    }
   ],
   "source": [
    "print(f'Training Score: {gs_pipe3.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {gs_pipe3.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b40a1-2e2d-4a9c-b149-cf379b6b426b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Stacked Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e66a0b5-b0c8-4818-be76-f8d39e047022",
   "metadata": {},
   "outputs": [],
   "source": [
    "lvl1_est_1 = [\n",
    "    ('nb', MultinomialNB()),\n",
    "    ('rf', RandomForestClassifier()),\n",
    "    ('ada', AdaBoostClassifier())\n",
    "]\n",
    "\n",
    "lvl1_est_2 = [\n",
    "    ('nb', MultinomialNB()),\n",
    "    ('rf', RandomForestClassifier()),\n",
    "    ('logr', LogisticRegression(max_iter=1000))\n",
    "]\n",
    "\n",
    "lvl1_est_3 = [\n",
    "    ('nb', MultinomialNB()),\n",
    "    ('logr', LogisticRegression(max_iter=1000)),\n",
    "    ('ada', AdaBoostClassifier())\n",
    "]\n",
    "\n",
    "lvl1_est_4 = [\n",
    "    ('logr', LogisticRegression(max_iter=1000)),\n",
    "    ('rf', RandomForestClassifier()),\n",
    "    ('ada', AdaBoostClassifier())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2d1599a-0d82-4f99-b738-1c564bfb8495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7266216542078612\n",
      "Training Score: 0.9506565857540787\n",
      "Testing Score: 0.737867939538584\n"
     ]
    }
   ],
   "source": [
    "stacked_1 = StackingClassifier(estimators=lvl1_est_1, \n",
    "                               final_estimator=LogisticRegression(),\n",
    "                               n_jobs = -1)\n",
    "\n",
    "pipe_tvec_1 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer(stop_words=stop_words_list)),\n",
    "    ('s1', stacked_1)\n",
    "])\n",
    "\n",
    "print(cross_val_score(pipe_tvec_1, X_train, y_train).mean())\n",
    "pipe_tvec_1.fit(X_train, y_train)\n",
    "print(f'Training Score: {pipe_tvec_1.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {pipe_tvec_1.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5d5fb01-bba7-4956-b5fb-c02b1aaa063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7276829261853173\n",
      "Training Score: 0.9416368218596631\n",
      "Testing Score: 0.733890214797136\n"
     ]
    }
   ],
   "source": [
    "stacked_2 = StackingClassifier(estimators=lvl1_est_2, \n",
    "                               final_estimator=LogisticRegression(),\n",
    "                               n_jobs = -1)\n",
    "pipe_tvec_2 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer(stop_words=stop_words_list)),\n",
    "    ('s2', stacked_2)\n",
    "])\n",
    "\n",
    "print(cross_val_score(pipe_tvec_2, X_train, y_train).mean())\n",
    "pipe_tvec_2.fit(X_train, y_train)\n",
    "print(f'Training Score: {pipe_tvec_2.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {pipe_tvec_2.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "265d2616-f00a-402e-afd1-20806cd526d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7282121980712466\n",
      "Training Score: 0.8670911261440509\n",
      "Testing Score: 0.7390612569610183\n"
     ]
    }
   ],
   "source": [
    "stacked_3 = StackingClassifier(estimators=lvl1_est_3, \n",
    "                               final_estimator=LogisticRegression(),\n",
    "                               n_jobs = -1)\n",
    "pipe_tvec_3 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer(stop_words=stop_words_list)),\n",
    "    ('s3', stacked_3)\n",
    "])\n",
    "\n",
    "print(cross_val_score(pipe_tvec_3, X_train, y_train).mean())\n",
    "pipe_tvec_3.fit(X_train, y_train)\n",
    "print(f'Training Score: {pipe_tvec_3.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {pipe_tvec_3.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6df5941-817a-49cc-a221-ed598bd36e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7260909742158169\n",
      "Training Score: 0.9429632577264889\n",
      "Testing Score: 0.7311058074781225\n"
     ]
    }
   ],
   "source": [
    "stacked_4 = StackingClassifier(estimators=lvl1_est_4, \n",
    "                               final_estimator=LogisticRegression(),\n",
    "                               n_jobs = -1)\n",
    "pipe_tvec_4 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer(stop_words=stop_words_list)),\n",
    "    ('s4', stacked_4)\n",
    "])\n",
    "\n",
    "print(cross_val_score(pipe_tvec_4, X_train, y_train).mean())\n",
    "pipe_tvec_4.fit(X_train, y_train)\n",
    "print(f'Training Score: {pipe_tvec_4.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {pipe_tvec_4.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02d790ae-6e97-46d0-b4dc-1fda765dab22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7302029960977858\n",
      "Training Score: 0.94455498076668\n",
      "Testing Score: 0.7374701670644391\n"
     ]
    }
   ],
   "source": [
    "stacked_1 = StackingClassifier(estimators=lvl1_est_1, \n",
    "                               final_estimator=LogisticRegression(),\n",
    "                               n_jobs = -1)\n",
    "pipe_cvec_1 = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words_list)),\n",
    "    ('s1', stacked_1)\n",
    "])\n",
    "\n",
    "print(cross_val_score(pipe_cvec_1, X_train, y_train).mean())\n",
    "pipe_cvec_1.fit(X_train, y_train)\n",
    "print(f'Training Score: {pipe_cvec_1.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {pipe_cvec_1.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c86c012c-6cdf-4eb8-99ab-d16e0e175bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7287435821163483\n",
      "Training Score: 0.943891762833267\n",
      "Testing Score: 0.7410501193317423\n"
     ]
    }
   ],
   "source": [
    "# BEST MODEL THUS FAR\n",
    "stacked_2 = StackingClassifier(estimators=lvl1_est_2, \n",
    "                               final_estimator=LogisticRegression(),\n",
    "                               n_jobs = -1)\n",
    "pipe_cvec_2 = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words_list)),\n",
    "    ('s2', stacked_2)\n",
    "])\n",
    "\n",
    "print(cross_val_score(pipe_cvec_2, X_train, y_train).mean())\n",
    "pipe_cvec_2.fit(X_train, y_train)\n",
    "print(f'Training Score: {pipe_cvec_2.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {pipe_cvec_2.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1f94209-07e8-4d73-863a-dfe5b1b7ee38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7248967242171369\n",
      "Training Score: 0.882345138612548\n",
      "Testing Score: 0.7410501193317423\n"
     ]
    }
   ],
   "source": [
    "stacked_3 = StackingClassifier(estimators=lvl1_est_3, \n",
    "                               final_estimator=LogisticRegression(),\n",
    "                               n_jobs = -1)\n",
    "pipe_cvec_3 = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words_list)),\n",
    "    ('s3', stacked_3)\n",
    "])\n",
    "\n",
    "print(cross_val_score(pipe_cvec_3, X_train, y_train).mean())\n",
    "pipe_cvec_3.fit(X_train, y_train)\n",
    "print(f'Training Score: {pipe_cvec_3.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {pipe_cvec_3.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "552e1555-8597-4c7e-8ef9-0cc392d7f0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7195904523364881\n",
      "Training Score: 0.9663085289826236\n",
      "Testing Score: 0.7307080350039777\n"
     ]
    }
   ],
   "source": [
    "stacked_4 = StackingClassifier(estimators=lvl1_est_4, \n",
    "                               final_estimator=LogisticRegression(),\n",
    "                               n_jobs = -1)\n",
    "pipe_cvec_4 = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words_list)),\n",
    "    ('s4', stacked_4)\n",
    "])\n",
    "\n",
    "print(cross_val_score(pipe_cvec_4, X_train, y_train).mean())\n",
    "pipe_cvec_4.fit(X_train, y_train)\n",
    "print(f'Training Score: {pipe_cvec_4.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {pipe_cvec_4.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f537c93e-3cbb-47f5-ac47-07ec47275b91",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The best model so far:\n",
    "> the Stacked Model with CountVectorizer as the transformer, the first level estimators based on a Multinomial NB, a Random Forest Classifier, and an Logistic Classifier, and the final estimator being another Logistic Regression Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc612b32-87da-4101-9682-ca235bb424b3",
   "metadata": {},
   "source": [
    "---\n",
    "## Gridsearching Through the First Level Estimators for Best Parameters\n",
    "\n",
    "### Starting With RandomSearch\n",
    "\n",
    "Going through each individual estimator in the first level of my stacking model to find the best hyperparameters. This will hopefully mean my final stacking model will be even better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "45640824-b650-4bcb-bcf1-3270165f4f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7034080568311628\n",
      "Best Params: {'nb__alpha': 1.0, 'cvec__ngram_range': (1, 1), 'cvec__min_df': 1, 'cvec__max_features': 2100, 'cvec__max_df': 0.9}\n",
      "Training Score: 0.7672105053720653\n",
      "Testing Score: 0.7048528241845664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "15 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1344, in fit_transform\n",
      "    raise ValueError(\"max_df corresponds to < documents than min_df\")\n",
      "ValueError: max_df corresponds to < documents than min_df\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.68603141 0.6788682  0.70340806 0.67780746 0.68815457        nan\n",
      " 0.69823582 0.684175          nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Starting with MultinomialNB\n",
    "pipe1 = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words_list)),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe1_params = {\n",
    "    'cvec__max_features' : range(500, 10001, 100),\n",
    "    'cvec__min_df': range(1, 16),\n",
    "    'cvec__max_df' : [0.9, 0.95, 0.98, 1],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (2,2), (1,3)],\n",
    "    'nb__alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "rs_pipe1 = RandomizedSearchCV(pipe1,\n",
    "                        param_distributions=pipe1_params,\n",
    "                        cv = 5)\n",
    "\n",
    "rs_pipe1.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best Score: {rs_pipe1.best_score_}')\n",
    "print(f'Best Params: {rs_pipe1.best_params_}')\n",
    "print(f'Training Score: {rs_pipe1.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {rs_pipe1.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be1243a1-d4fa-424e-b421-9893c71ffa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.709112382709161\n",
      "Best Params: {'cvec__max_df': 0.1, 'cvec__max_features': 4000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 1), 'nb__alpha': 0.7}\n",
      "Training Score: 0.8047486404032365\n",
      "Testing Score: 0.7143993635640413\n"
     ]
    }
   ],
   "source": [
    "pipe1 = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words_list)),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe1_params = {\n",
    "    'cvec__max_features' : [3000, 4000, 5000],\n",
    "    'cvec__min_df': [2],\n",
    "    'cvec__max_df' : [0.01, 0.05, 0.1, 0.15],\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'nb__alpha': [0.5, 0.6, 0.7]\n",
    "}\n",
    "\n",
    "gs_pipe1 = GridSearchCV(pipe1,\n",
    "                        param_grid=pipe1_params,\n",
    "                        cv = 5,\n",
    "                        n_jobs= -1)\n",
    "\n",
    "gs_pipe1.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best Score: {gs_pipe1.best_score_}')\n",
    "print(f'Best Params: {gs_pipe1.best_params_}')\n",
    "print(f'Training Score: {gs_pipe1.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {gs_pipe1.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd08160-a957-4ef6-aeb9-c85753151773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cd9fbdd7-b225-401a-bfe8-2703db33bbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7072550907436383\n",
      "Best Params: {'rf__n_estimators': 5700, 'rf__min_samples_split': 22, 'rf__min_samples_leaf': 2, 'rf__max_depth': 74, 'cvec__ngram_range': (1, 1), 'cvec__min_df': 4, 'cvec__max_features': 8200, 'cvec__max_df': 0.98}\n",
      "Training Score: 0.7445284520493434\n",
      "Testing Score: 0.6984884645982498\n"
     ]
    }
   ],
   "source": [
    "pipe2 = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words_list)),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipe2_params = {\n",
    "    'cvec__max_features' : [8000, 8200, 8400],\n",
    "    'cvec__min_df': [3, 4, 5],\n",
    "    'cvec__max_df' : [0.9, 0.95, 0.98],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'rf__n_estimators': range(50, 10001, 50),\n",
    "    'rf__max_depth': range(2, 101, 2),\n",
    "    'rf__min_samples_leaf': range(1, 21, 1),\n",
    "    'rf__min_samples_split': range(2, 101, 2)\n",
    "}\n",
    "\n",
    "rs_pipe2 = RandomizedSearchCV(pipe2,\n",
    "                              param_distributions=pipe2_params,\n",
    "                              cv = 5,\n",
    "                              n_jobs = -1)\n",
    "\n",
    "rs_pipe2.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best Score: {rs_pipe2.best_score_}')\n",
    "print(f'Best Params: {rs_pipe2.best_params_}')\n",
    "print(f'Training Score: {rs_pipe2.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {rs_pipe2.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4ad03e-057d-49f8-b452-f538ea4cd78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words_list)),\n",
    "    ('rf', RandomForestClassifier(n_jobs = -1))\n",
    "])\n",
    "\n",
    "pipe2_params = {\n",
    "    'cvec__max_features' : [8000, 8200, 8400],\n",
    "    'cvec__min_df': [3, 4, 5],\n",
    "    'cvec__max_df' : [0.9, 0.95, 0.98],\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'rf__n_estimators': [5600, 5700, 5800],\n",
    "    'rf__max_depth': [72,74,76],\n",
    "    'rf__min_samples_leaf': [1, 2, 3],\n",
    "    'rf__min_samples_split': [3, 4, 5],\n",
    "    'rf__ccp_alpha': [0.01, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "gs_pipe2 = GridSearchCV(pipe2,\n",
    "                        param_grid=pipe2_params,\n",
    "                        cv = 5,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "gs_pipe2.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best Score: {gs_pipe2.best_score_}')\n",
    "print(f'Best Params: {gs_pipe2.best_params_}')\n",
    "print(f'Training Score: {gs_pipe2.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {gs_pipe2.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba1b247a-583d-4a97-a03b-46531aa011ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7259577321746967\n",
      "Best Params: {'log__C': 0.08, 'log__penalty': 'l2', 'log__solver': 'sag'}\n",
      "Training Score: 0.8076667993102533\n",
      "Testing Score: 0.7303102625298329\n"
     ]
    }
   ],
   "source": [
    "pipe3 = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words_list)),\n",
    "    ('log', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe3_params = {\n",
    "    'log__penalty': ['l2'],\n",
    "    'log__solver': ['newton-cg', 'sag', 'lbfgs'],\n",
    "    'log__C': [0.07, 0.08, 0.09]\n",
    "}\n",
    "\n",
    "gs_pipe3 = GridSearchCV(pipe3,\n",
    "                        param_grid=pipe3_params,\n",
    "                        cv = 5,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "gs_pipe3.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best Score: {gs_pipe3.best_score_}')\n",
    "print(f'Best Params: {gs_pipe3.best_params_}')\n",
    "print(f'Training Score: {gs_pipe3.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {gs_pipe3.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1361181-ce77-456d-8747-083121b54955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7114993865937737\n",
      "Best Params: {'log__C': 1, 'log__penalty': 'l1', 'log__solver': 'saga'}\n",
      "Training Score: 0.8188088605915904\n",
      "Testing Score: 0.7143993635640413\n"
     ]
    }
   ],
   "source": [
    "pipe3 = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words_list)),\n",
    "    ('log', LogisticRegression(max_iter = 10000))\n",
    "])\n",
    "\n",
    "pipe3_params = {\n",
    "    'log__penalty': ['l1'],\n",
    "    'log__solver': ['liblinear', 'saga'],\n",
    "    'log__C': [0.9, 1, 1.1]\n",
    "}\n",
    "\n",
    "gs_pipe3 = GridSearchCV(pipe3,\n",
    "                        param_grid=pipe3_params,\n",
    "                        cv = 5,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "gs_pipe3.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best Score: {gs_pipe3.best_score_}')\n",
    "print(f'Best Params: {gs_pipe3.best_params_}')\n",
    "print(f'Training Score: {gs_pipe3.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {gs_pipe3.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56df512a-0643-41ed-9a54-c529f44419b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7271520701800087\n",
      "Training Score: 0.8182782862448601\n",
      "Testing Score: 0.7374701670644391\n"
     ]
    }
   ],
   "source": [
    "lvl1_est = [\n",
    "    ('nb', MultinomialNB(alpha = 0.7)),\n",
    "    ('rf', RandomForestClassifier(max_depth = 92, \n",
    "                                  min_samples_leaf = 1,\n",
    "                                  min_samples_split = 5, \n",
    "                                  n_estimators = 50,\n",
    "                                  ccp_alpha = 0.01)),\n",
    "    ('log', LogisticRegression(penalty='l2', \n",
    "                               solver='sag', \n",
    "                               C = 0.08))\n",
    "]\n",
    "\n",
    "stacked_1 = StackingClassifier(estimators=lvl1_est, \n",
    "                               final_estimator=LogisticRegression(),\n",
    "                               n_jobs = -1)\n",
    "\n",
    "pipe_cvec_1 = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words_list,\n",
    "                            max_df=0.1,\n",
    "                            min_df = 2,\n",
    "                            max_features=4000,\n",
    "                            ngram_range= (1, 1))),\n",
    "    ('s1', stacked_1)\n",
    "])\n",
    "\n",
    "print(cross_val_score(pipe_cvec_1, X_train, y_train).mean())\n",
    "pipe_cvec_1.fit(X_train, y_train)\n",
    "print(f'Training Score: {pipe_cvec_1.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {pipe_cvec_1.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dffc24c-4c88-4301-89de-9303c1a7be7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4317e0a-4d97-4794-b685-3df3c671bf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "30 fits failed out of a total of 120.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py\", line 488, in fit\n",
      "    return super().fit(X, self._le.transform(y), sample_weight)\n",
      "  File \"C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py\", line 217, in fit\n",
      "    _fit_single_estimator(\n",
      "  File \"C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py\", line 42, in _fit_single_estimator\n",
      "    estimator.fit(X, y)\n",
      "  File \"C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py\", line 488, in fit\n",
      "    return super().fit(X, self._le.transform(y), sample_weight)\n",
      "  File \"C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py\", line 217, in fit\n",
      "    _fit_single_estimator(\n",
      "  File \"C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py\", line 42, in _fit_single_estimator\n",
      "    estimator.fit(X, y)\n",
      "  File \"C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\jeffr\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.72662174        nan 0.72675419 0.72662139 0.72662139\n",
      " 0.7267541  0.72502935        nan 0.72861096        nan 0.72847824\n",
      " 0.72715207 0.72728478 0.72728478 0.72728478        nan 0.72834562\n",
      "        nan 0.72847833 0.72794765 0.72794765 0.72834562 0.72794765]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7286109561216533\n",
      "Best Params: {'s1__final_estimator__C': 1, 's1__final_estimator__penalty': 'l1', 's1__final_estimator__solver': 'saga'}\n",
      "Training Score: 0.8177477118981297\n",
      "Testing Score: 0.7390612569610183\n"
     ]
    }
   ],
   "source": [
    "lvl1_est = [\n",
    "    ('nb', MultinomialNB(alpha = 0.7)),\n",
    "    ('rf', RandomForestClassifier(max_depth = 92, \n",
    "                                  min_samples_leaf = 1,\n",
    "                                  min_samples_split = 5, \n",
    "                                  n_estimators = 50,\n",
    "                                  ccp_alpha = 0.01)),\n",
    "    ('log', LogisticRegression(penalty='l2', \n",
    "                               solver='sag', \n",
    "                               C = 0.08))\n",
    "]\n",
    "\n",
    "stacked_1 = StackingClassifier(estimators=lvl1_est, \n",
    "                               final_estimator=LogisticRegression(),\n",
    "                               n_jobs = -1)\n",
    "\n",
    "pipe_final = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words_list,\n",
    "                            max_df=0.1,\n",
    "                            min_df = 2,\n",
    "                            max_features=4000,\n",
    "                            ngram_range= (1, 1))),\n",
    "    ('s1', stacked_1)\n",
    "])\n",
    "\n",
    "\n",
    "pipe_final_params = {\n",
    "    's1__final_estimator__penalty': ['l1', 'l2'],\n",
    "    's1__final_estimator__solver': ['lbfgs', 'saga', 'sag', 'liblinear'],\n",
    "    's1__final_estimator__C': [0.1, 1, 2]\n",
    "}\n",
    "\n",
    "gs_final = GridSearchCV(pipe_final,\n",
    "                        param_grid= pipe_final_params,\n",
    "                        cv = 5,\n",
    "                        n_jobs = -1)\n",
    "    \n",
    "    \n",
    "gs_final.fit(X_train, y_train)\n",
    "print(f'Best Score: {gs_final.best_score_}')\n",
    "print(f'Best Params: {gs_final.best_params_}')\n",
    "print(f'Training Score: {gs_final.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {gs_final.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84d37c-bde8-4103-a1d0-3500547b5117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
