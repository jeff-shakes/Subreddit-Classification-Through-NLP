{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c91f8288-eb48-415f-acaa-6c705c991860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b34a89ab-ed77-4f24-b3ff-e3bb4b66da00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ufc_mma_submissions.csv')\n",
    "\n",
    "X = df['title']\n",
    "y = df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcaeea20-81bd-4ae6-918d-1b864186c511",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_words_list= (['ufc', 'dana', 'white', 'ultimate', 'u.f.c.', 'islam', 'makhachev', 'moreno', 'edwards', 'usman', 'ngannou', 'adesanya',\n",
    "                  'pantoja', 'kara', 'kai', 'oliveira', 'pereira', 'sterling', 'royval', 'nicolau', 'perez', 'albazi', 'schnell', 'omalley', 'yan',\n",
    "                  'dvalishvili', 'vera', 'sandhagen', 'font', 'cruz', 'holloway', 'volkanovski', 'figueiredo', 'deiveson', 'aljamain', 'rodriguez',\n",
    "                  'ortega', 'allen', 'emmett', 'chan', 'sung', 'jung', 'kattar', 'giga', 'chikadze', 'poirier', 'jones', 'elliott', 'dvorak', 'molina', 'mokaev',\n",
    "                  'ulanbekov', 'yanez', 'gutierrez', 'nurmagomedov', 'simon', 'munhoz', 'shore', 'topuria', 'evloev', 'mitchell', 'yusuff', 'iga', 'barboza',\n",
    "                  'caceres', 'burns', 'neal', 'luque', 'fiziev', 'gamrot', 'anjos', 'tsarukyan', 'turner', 'hooker', 'ismagulov', 'gaethje', 'magny', 'whittaker',\n",
    "                  'vettori', 'strickland', 'costa', 'hermansson', 'covington', 'muniz', 'imavov', 'bachowicz', 'rakic', 'cannonier', 'dolidze', 'brunson', 'oezdemir',\n",
    "                  'spann', 'walker', 'nunes', 'weili', 'shevchenko', 'pena', 'blaydes', 'tuivasa', 'aspinall', 'andrade', 'santos', 'daukaus', 'tybura', 'lewis', 'holm',\n",
    "                  'vieira', 'jandiroba', 'maia', 'grasso', 'chookagian', 'murphy', 'fiorot', 'lemos', 'namajunas', 'esparza', 'jandiroba', 'blanchfield', 'barber',\n",
    "                  'calvillo', 'ribas', 'viana', 'ducote', 'pinheiro', 'xiaonan', 'yan', 'abdurakhimov', 'spivac', 'shamil', 'ketlen', 'pennington', 'miesha', 'kunitskaya',\n",
    "                  'rosa', 'avila', 'lansberg', 'paddy', 'silva', 'cormier', 'diaz', 'miocic', 'lesnar', 'penn', 'liddell', 'pierre', 'rousey', 'khabib', 'conor', 'mcgregor',\n",
    "                  'frevola', 'dillashaw', 'pimblett', 'helwani', 'blachowicz','arlovski', 'donatello', 'dec', 'december', 'jan', 'feb', 'selftext', 'says', 'did', 'does',\n",
    "                  'guy', 'guys', 'know', 'fc', 'vs', 'https', 'khamzat', '2022', '2023', '219', '281', '282', '283', '284', '285', 'going', 'man', 'got', 'anne', 'didnt', \n",
    "                  'ufc281', 'ankalaev', 'zhang', 'israel', 'johnson', 'dustin', 'krause', 'chandler', 'jiri', 'cejudo', 'march', 'februrary', 'gordon', 'ilia', 'florian',\n",
    "                  'makachov', 'beneil', 'dariush', 'jared'])\n",
    "stop_words_list = text.ENGLISH_STOP_WORDS.union(my_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8d8944f-d7ef-4934-9cac-d8d388cec1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7282135181707293\n",
      "Training Score: 0.943891762833267\n",
      "Testing Score: 0.7402545743834527\n"
     ]
    }
   ],
   "source": [
    "#Best Model from Previous Investigation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,\n",
    "                                                   stratify = y)\n",
    "\n",
    "lvl1_est_1 = [\n",
    "    ('nb', MultinomialNB()),\n",
    "    ('rf', RandomForestClassifier()),\n",
    "    ('logr', LogisticRegression(max_iter = 1000))\n",
    "]\n",
    "\n",
    "stacked_2 = StackingClassifier(estimators=lvl1_est_1, \n",
    "                               final_estimator=LogisticRegression(),\n",
    "                               n_jobs = -1)\n",
    "pipe_cvec_2 = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words_list)),\n",
    "    ('s2', stacked_2)\n",
    "])\n",
    "\n",
    "print(cross_val_score(pipe_cvec_2, X_train, y_train).mean())\n",
    "pipe_cvec_2.fit(X_train, y_train)\n",
    "print(f'Training Score: {pipe_cvec_2.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {pipe_cvec_2.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515582e6-c3fb-4fac-b22b-d7dcc10090dc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c672c14-1c81-4913-b3a2-e6c28daeecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Got a LOT of help from: https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def lem_word_wnl(string):\n",
    "    '''\n",
    "    This function splits a string on spaces, uses .lemmatize from wnl on each word\n",
    "    of the string, and then rejoins the string. For future vectorizing and modeling.\n",
    "    '''\n",
    "    string = string.split()\n",
    "    return ' '.join([wnl.lemmatize(i) for i in string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5da4127-c421-467f-8bce-a48c89414443",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X.map(lambda i: lem_word_wnl(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5eb17be-1052-4f4b-b428-8ddba927c66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     UFC Fight Pass Streaming Quality\n",
       "1                    Will UFC 284 sell 1,000,000 PPVs?\n",
       "2    The ONE Championship team and CEO Chatri Sityo...\n",
       "3                                       Yoel look tiny\n",
       "4       Who is your favorite prospect going into 2023?\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6671508f-a6cd-4184-8925-e4465085dd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7298052941269654\n",
      "Training Score: 0.9478710704337445\n",
      "Testing Score: 0.7390612569610183\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X1, y, random_state=42,\n",
    "                                                   stratify = y)\n",
    "\n",
    "lvl1_est_1 = [\n",
    "    ('nb', MultinomialNB()),\n",
    "    ('rf', RandomForestClassifier()),\n",
    "    ('logr', LogisticRegression(max_iter = 1000))\n",
    "]\n",
    "\n",
    "stacked_2 = StackingClassifier(estimators=lvl1_est_1, \n",
    "                               final_estimator=LogisticRegression(),\n",
    "                               n_jobs = -1)\n",
    "pipe_cvec_2 = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words_list)),\n",
    "    ('s2', stacked_2)\n",
    "])\n",
    "\n",
    "print(cross_val_score(pipe_cvec_2, X_train, y_train).mean())\n",
    "pipe_cvec_2.fit(X_train, y_train)\n",
    "print(f'Training Score: {pipe_cvec_2.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {pipe_cvec_2.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc01d8e-1ca6-474e-a5bd-2ba90774dabd",
   "metadata": {},
   "source": [
    "####\n",
    "### After Running the same, Best Model:\n",
    "\n",
    "##### It seems the lemmatized titles do not do any better (in fact, ever so slightly worse -- but easily could be b/c of randomness) than the non-lemmatized titles\n",
    "\n",
    "---\n",
    "####\n",
    "\n",
    "### Parts of Speech Investigation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ec7faa8-dd18-4bce-abbb-df8b3c9e8502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_pos_tag(string):\n",
    "    '''\n",
    "    This function uses the .pos_tag to add up the different parts-of-speech totals\n",
    "    in each string. Then returns those counts so they can be made into different\n",
    "    columns of a new dataframe.\n",
    "    '''\n",
    "    string = string.split()\n",
    "    adj = 0\n",
    "    verb = 0\n",
    "    noun = 0\n",
    "    adv = 0\n",
    "    for i in nltk.pos_tag(string):\n",
    "        if i[1][0] == 'J':\n",
    "            adj += 1\n",
    "        elif i[1][0] == 'V':\n",
    "            verb += 1\n",
    "        elif i[1][0] == 'N':\n",
    "            noun += 1\n",
    "        elif i[1][0] == 'R':\n",
    "            adv += 1\n",
    "        else:\n",
    "            pass\n",
    "    return [adj, verb, noun, adv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc06a10d-5be5-4925-b886-0522dd7f83f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl_df = pd.DataFrame(data = y, index = X.index)\n",
    "wnl_df['adj_num'] = X.map(lambda j: return_pos_tag(j)[0])\n",
    "wnl_df['verb_num'] = X.map(lambda j: return_pos_tag(j)[1])\n",
    "wnl_df['noun_num'] = X.map(lambda j: return_pos_tag(j)[2])\n",
    "wnl_df['adv_num'] = X.map(lambda j: return_pos_tag(j)[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cc3c53e-fd73-44a6-969f-8bbeaaf8229c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>adj_num</th>\n",
       "      <th>verb_num</th>\n",
       "      <th>noun_num</th>\n",
       "      <th>adv_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit  adj_num  verb_num  noun_num  adv_num\n",
       "0          1        0         0         5        0\n",
       "1          1        0         1         2        0\n",
       "2          0        0         0        12        0\n",
       "3          1        1         1         1        0\n",
       "4          1        1         2         1        0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1da095-07d5-449f-a54c-20c4b7db342e",
   "metadata": {
    "tags": []
   },
   "source": [
    "##\n",
    "---\n",
    "#### Modeling on JUST the Parts of Speech Totals\n",
    "\n",
    "##### Interested to see if does better than Null Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f14c26d6-411f-47d1-88b2-d951ab547632",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wnl_df.drop(columns = 'subreddit')\n",
    "y = wnl_df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c964348-9408-42ec-b097-be74da142d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6381483228576107\n",
      "Training Score: 0.6906751558562143\n",
      "Testing Score: 0.6471758154335719\n"
     ]
    }
   ],
   "source": [
    "lvl1_est_1 = [\n",
    "    ('nb', MultinomialNB()),\n",
    "    ('rf', RandomForestClassifier()),\n",
    "    ('ada', AdaBoostClassifier())\n",
    "]\n",
    "\n",
    "stacked_1 = StackingClassifier(estimators=lvl1_est_1, final_estimator=LogisticRegression())\n",
    "\n",
    "\n",
    "print(cross_val_score(stacked_1, X_train, y_train).mean())\n",
    "stacked_1.fit(X_train, y_train)\n",
    "print(f'Training Score: {stacked_1.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {stacked_1.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b50d2-c2ca-4c2c-baef-df5e558a9b93",
   "metadata": {},
   "source": [
    "###\n",
    "---\n",
    "#### These scores reflect only using the sum of the parts of speech for each title for classification. \n",
    "\n",
    "##### The model is approximately 14% more accurate with just these parts of speech than the baseline accuracy. Therefore, these features will be included in future modeling.  \n",
    "\n",
    "---\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc6e53ef-75ff-40bc-a3c0-1fcd3e70139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new dataframe to include the POS counts and adding crucial columns\n",
    "    #of the original df\n",
    "new_df= pd.DataFrame(data = X1, index = X1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "718ba715-a28a-40ba-b5ac-78488cdd156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['adj_num'] = wnl_df['adj_num']\n",
    "new_df['verb_num'] = wnl_df['verb_num']\n",
    "new_df['noun_num'] = wnl_df['noun_num']\n",
    "new_df['adv_num'] = wnl_df['adv_num']\n",
    "new_df['subreddit'] = df['subreddit']\n",
    "new_df['selftext'] = df['selftext']\n",
    "new_df['word_count'] = df['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "787a61d6-b5c8-44b8-85d9-7b97b9fea264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>adj_num</th>\n",
       "      <th>verb_num</th>\n",
       "      <th>noun_num</th>\n",
       "      <th>adv_num</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UFC Fight Pass Streaming Quality</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will UFC 284 sell 1,000,000 PPVs?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The ONE Championship team and CEO Chatri Sityo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yoel look tiny</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who is your favorite prospect going into 2023?</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  adj_num  verb_num  \\\n",
       "0                   UFC Fight Pass Streaming Quality        0         0   \n",
       "1                  Will UFC 284 sell 1,000,000 PPVs?        0         1   \n",
       "2  The ONE Championship team and CEO Chatri Sityo...        0         0   \n",
       "3                                     Yoel look tiny        1         1   \n",
       "4     Who is your favorite prospect going into 2023?        1         2   \n",
       "\n",
       "   noun_num  adv_num  subreddit  selftext  word_count  \n",
       "0         5        0          1         0           5  \n",
       "1         2        0          1         0           6  \n",
       "2        12        0          0         0          18  \n",
       "3         1        0          1         0           3  \n",
       "4         1        0          1         0           8  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d13f195-3bad-4962-8fec-23230870490f",
   "metadata": {},
   "source": [
    "#####\n",
    "---\n",
    "#####\n",
    "#### Applying same stacked model as was found as the best from previous notebook to see if POS helps the overall model\n",
    "\n",
    ">I have to include a column transformer so CountVectorizer is only applied to the 'title' column  \n",
    "---\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e049aea-8892-453f-9beb-ebe2eda07921",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = new_df.drop(columns = 'subreddit')\n",
    "y1 = new_df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, stratify = y1, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d23f293-0c38-4e00-9315-b6877049bc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7290099781919566\n",
      "Training Score: 0.9482690011937923\n",
      "Testing Score: 0.7386634844868735\n"
     ]
    }
   ],
   "source": [
    "lvl1_est_1 = [\n",
    "    ('nb', MultinomialNB()),\n",
    "    ('rf', RandomForestClassifier()),\n",
    "    ('logr', LogisticRegression(max_iter = 1000))\n",
    "]\n",
    "\n",
    "stacked_2 = StackingClassifier(estimators=lvl1_est_1, \n",
    "                               final_estimator=LogisticRegression(),\n",
    "                               n_jobs = -1)\n",
    "ct = ColumnTransformer([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words_list), 'title')\n",
    "])\n",
    "\n",
    "pipe_cvec_2 = Pipeline([\n",
    "    ('ct', ct),\n",
    "    ('s2', stacked_2)\n",
    "])\n",
    "\n",
    "print(cross_val_score(pipe_cvec_2, X_train, y_train).mean())\n",
    "pipe_cvec_2.fit(X_train, y_train)\n",
    "print(f'Training Score: {pipe_cvec_2.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {pipe_cvec_2.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d119d444-be95-44fa-a669-b3f24ca053e1",
   "metadata": {},
   "source": [
    "Adding the POS to the model seems to have made the model slightly worse. However, this could be due to randomness inherent in the random states that weren't hardcoded. The POS will remain for further analysis later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d0b92-b663-42d0-8c18-5b6cb12ed7cd",
   "metadata": {},
   "source": [
    "####\n",
    "---\n",
    "### Sentiment Analysis\n",
    "\n",
    "##### Using SentimentIntensityAnalyzer for the Negative, Neutral, Positive, and Compound Sentiment Scores on the 'Title' column\n",
    "---\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ad69143-89ce-4026-9173-1d18292f166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "732e7723-60d4-481d-8fb0-67b665e3d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['neg_sent_score'] = df['title'].map(lambda i: list(sent.polarity_scores(i).values())[0])\n",
    "new_df['neutral_sent_score'] = df['title'].map(lambda i: list(sent.polarity_scores(i).values())[1])\n",
    "new_df['pos_sent_score'] = df['title'].map(lambda i: list(sent.polarity_scores(i).values())[2])\n",
    "new_df['cmpd_sent_score'] = df['title'].map(lambda i: list(sent.polarity_scores(i).values())[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3cc34b3-ec7d-4db2-a6ec-5d10f39f7836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>adj_num</th>\n",
       "      <th>verb_num</th>\n",
       "      <th>noun_num</th>\n",
       "      <th>adv_num</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>word_count</th>\n",
       "      <th>neg_sent_score</th>\n",
       "      <th>neutral_sent_score</th>\n",
       "      <th>pos_sent_score</th>\n",
       "      <th>cmpd_sent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UFC Fight Pass Streaming Quality</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.3818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will UFC 284 sell 1,000,000 PPVs?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The ONE Championship team and CEO Chatri Sityo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yoel look tiny</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who is your favorite prospect going into 2023?</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.6369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  adj_num  verb_num  \\\n",
       "0                   UFC Fight Pass Streaming Quality        0         0   \n",
       "1                  Will UFC 284 sell 1,000,000 PPVs?        0         1   \n",
       "2  The ONE Championship team and CEO Chatri Sityo...        0         0   \n",
       "3                                     Yoel look tiny        1         1   \n",
       "4     Who is your favorite prospect going into 2023?        1         2   \n",
       "\n",
       "   noun_num  adv_num  subreddit  selftext  word_count  neg_sent_score  \\\n",
       "0         5        0          1         0           5           0.394   \n",
       "1         2        0          1         0           6           0.000   \n",
       "2        12        0          0         0          18           0.000   \n",
       "3         1        0          1         0           3           0.000   \n",
       "4         1        0          1         0           8           0.000   \n",
       "\n",
       "   neutral_sent_score  pos_sent_score  cmpd_sent_score  \n",
       "0               0.606           0.000          -0.3818  \n",
       "1               1.000           0.000           0.0000  \n",
       "2               0.854           0.146           0.4404  \n",
       "3               1.000           0.000           0.0000  \n",
       "4               0.536           0.464           0.6369  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3900da08-22de-4fd9-ac76-88e2aa04ca69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10053, 12)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d8ef49-0676-483c-8ac1-524007f8e6e7",
   "metadata": {},
   "source": [
    "###\n",
    "Using the same model as before but now with the Sentiment Analysis columns added to the dataframe. \n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d3e64c4-2d40-47f8-b201-09389eade1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7304686001137046\n",
      "Training Score: 0.9494627934739356\n",
      "Testing Score: 0.7402545743834527\n"
     ]
    }
   ],
   "source": [
    "X1 = new_df.drop(columns = 'subreddit')\n",
    "y1 = new_df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, stratify = y1, \n",
    "                                                    random_state=42)\n",
    "\n",
    "lvl1_est_1 = [\n",
    "    ('nb', MultinomialNB()),\n",
    "    ('rf', RandomForestClassifier()),\n",
    "    ('logr', LogisticRegression(max_iter = 1000))\n",
    "]\n",
    "\n",
    "stacked_2 = StackingClassifier(estimators=lvl1_est_1, \n",
    "                               final_estimator=LogisticRegression(),\n",
    "                               n_jobs = -1)\n",
    "ct = ColumnTransformer([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words_list), 'title')\n",
    "])\n",
    "\n",
    "pipe_cvec_2 = Pipeline([\n",
    "    ('ct', ct),\n",
    "    ('s2', stacked_2)\n",
    "])\n",
    "\n",
    "print(cross_val_score(pipe_cvec_2, X_train, y_train).mean())\n",
    "pipe_cvec_2.fit(X_train, y_train)\n",
    "print(f'Training Score: {pipe_cvec_2.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {pipe_cvec_2.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480b739f-3b49-4f69-a880-67afd7369482",
   "metadata": {},
   "source": [
    "####\n",
    "Again, adding the Sentiment Analysis to the model seems to have made the model slightly worse. However, this still could be due to randomness inherent in the random states that weren't hardcoded. Plus, no parameters were tuned to potentially make this model better (specifically cvec).  \n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb50e08a-cec1-40bc-9f53-6218793b6103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I was having trouble with git due to a size issue. So to make sure, I am splitting\n",
    "    # this dataframe in half when exporting\n",
    "first_half = new_df.loc[0:5000]\n",
    "second_half = new_df.loc[5001:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "faa8bd1b-6e3b-4e9a-8942-4bc5b77ac89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_half.to_csv('../data/new_data_part1.csv', index = False)\n",
    "second_half.to_csv('../data/new_data_part2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c73a7-bf07-4bde-82d1-6b598ab7768e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
